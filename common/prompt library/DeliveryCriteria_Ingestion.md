# Delivery Criteria - Ingestion Projects (Days 1A to 1E)

## üìã MATRIZ DE MICRO-DECIS√ïES

### ‚úÖ OUTPUT OBRIGAT√ìRIO (Voc√™ DEVE entregar isso)

| Item | Descri√ß√£o | Localiza√ß√£o |
|------|-----------|-------------|
| **Dados estruturados** | CSV/JSON/Parquet com schema documentado | `/dayXX/data/processed/` ou BigQuery |
| **Script de extra√ß√£o** | `dayXX_DATA_extract.py` execut√°vel sem erros | `/dayXX/dayXX_DATA_extract.py` |
| **Script de loading** | `dayXX_DATA_load.py` salva dados no destino | `/dayXX/dayXX_DATA_load.py` |
| **Configura√ß√£o reproduz√≠vel** | `.env.example` com vari√°veis DAY espec√≠ficas | `/dayXX/.env.example` |
| **Depend√™ncias** | Day-specific requirements (se necess√°rio) | `/dayXX/dayXX_requirements.txt` |
| **Documenta√ß√£o m√≠nima** | README com Quick Start copy-paste | `/dayXX/README.md` |

### ‚ùå OUTPUT PROIBIDO (N√ÉO fa√ßa isso - vai estourar 3h)

| Item | Por que N√ÉO |
|------|-------------|
| **Dashboards/visualiza√ß√µes** | Isso √© Pilar D (Dashboard) |
| **Orquestra√ß√£o/scheduling** | Isso √© Pilar C (Orchestration) |
| **An√°lises/insights sobre os dados** | Isso √© Pilar E (AI Insights) |
| **Modelagem de dados** | Isso √© Pilar B (Modeling) |
| **Testes end-to-end complexos** | S√≥ testes unit√°rios b√°sicos (< 30 min) |
| **Otimiza√ß√µes prematuras** | Performance vem depois, funcionalidade primeiro |
| **UI/Frontend** | √â CLI ou script Python, ponto final |

---

## ‚úÖ CHECKLIST DE "PROJETO COMPLETO"

### **ANTES de considerar o projeto pronto, verifique:**

#### **1. Funcionalidade Core**
- [ ] `python dayXX_DATA_extract.py` executa sem erros
- [ ] Dados aparecem em `/dayXX/data/processed/` (ou BigQuery) com estrutura esperada
- [ ] Schema est√° documentado (README ou comments no c√≥digo)

#### **2. Reprodutibilidade** (respeitando regras do PROMPT_project_setup.md)
- [ ] `.env.example` lista TODAS as vari√°veis necess√°rias (formato: `KEY_OPENAI_DAYXX`, `DAYXX_SPECIFIC_VAR`)
- [ ] Vari√°veis adicionadas ao root `config/.env` seguindo conven√ß√£o existente
- [ ] `dayXX_requirements.txt` tem depend√™ncias espec√≠ficas (se necess√°rio)
- [ ] README tem se√ß√£o "Quick Start" que funciona copy-paste

#### **3. Qualidade M√≠nima**
- [ ] C√≥digo tem docstrings nas fun√ß√µes principais
- [ ] H√° tratamento de erros b√°sico (try/except em chamadas de API)
- [ ] Logs informativos (print ou logging mostrando progresso)

#### **4. Nomenclatura (CR√çTICO)**
- [ ] Todos os arquivos t√™m prefixo `dayXX_`
- [ ] Vari√°veis globais t√™m prefixo `dayXX_` ou `DAYXX_`
- [ ] Classes t√™m prefixo `dayXX_`
- [ ] Fun√ß√µes principais t√™m prefixo `dayXX_`

#### **5. Entrega**
- [ ] Git commit com mensagem descritiva
- [ ] Push para GitHub

#### **6. Teste Final (5 minutos)**
- [ ] Clone o repo em outra pasta
- [ ] Rode os comandos do README
- [ ] Funciona? ‚úÖ Projeto completo. N√£o funciona? ‚ùå Debug e fix.

---

## ‚è±Ô∏è GEST√ÉO DE TEMPO (3 horas)

### **Distribui√ß√£o ideal:**

| Fase | Tempo | O que fazer |
|------|-------|-------------|
| **Setup** | 20 min | Criar pasta dayXX/, copiar estrutura base, configurar .env |
| **Extract** | 90 min | Implementar `dayXX_DATA_extract.py`, testar API, tratar erros |
| **Load** | 40 min | Implementar `dayXX_DATA_load.py`, salvar dados |
| **Docs** | 20 min | README Quick Start + docstrings |
| **Buffer** | 10 min | Imprevistos, ajustes finais |

### **üö® SINAIS DE QUE VOC√ä EST√Å DESVIANDO DO ESCOPO:**

- Voc√™ est√° escrevendo CSS
- Voc√™ est√° criando um dashboard
- Voc√™ est√° fazendo an√°lise explorat√≥ria dos dados
- Voc√™ est√° otimizando performance antes de funcionar
- Voc√™ est√° adicionando "features legais" n√£o solicitadas

**Se isso acontecer:** PARE. Volte aos requisitos. Entregue o m√≠nimo prometido primeiro.

---

## üéØ CRIT√âRIOS ESPEC√çFICOS POR PROJETO

### **Day 1A (Daud - GA4 + Google Ads)**

**Fontes de dados:**
- GA4 Demo Account (Google Merchandise Store) OU dados sint√©ticos (backup ap√≥s 1h)
- Google Ads dados sint√©ticos (no free sandbox available)

**Output obrigat√≥rio:**
- [ ] Tabela `ga4_sessions` em BigQuery: `date, sessions, conversions, bounce_rate, source`
- [ ] Tabela `google_ads_campaigns` em BigQuery: `date, campaign_name, spend, clicks, impressions, conversions`
- [ ] Join preparado (mesmo date field) para an√°lise futura
- [ ] Schema documentado no README

**Quando parar:**
- ‚úÖ Dados de GA4 (real ou sint√©tico) em BigQuery
- ‚úÖ Dados de Google Ads (sint√©tico) em BigQuery
- ‚úÖ README explica como conectar GA4 real vs. sint√©tico
- ‚ùå N√ÉO fa√ßa: Dashboard de ROAS, an√°lise de campanhas, alertas

**Arquivos esperados:**
```
dayXX/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ga4_sample.json (se Demo Account)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ads_synthetic.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ ga4_sessions.csv
‚îÇ       ‚îî‚îÄ‚îÄ ads_campaigns.csv
‚îú‚îÄ‚îÄ dayXX_DATA_extract_ga4.py
‚îú‚îÄ‚îÄ dayXX_DATA_extract_ads.py (synthetic generator)
‚îú‚îÄ‚îÄ dayXX_DATA_load_bigquery.py
‚îú‚îÄ‚îÄ dayXX_CONFIG_settings.py
‚îú‚îÄ‚îÄ dayXX_requirements.txt (se necess√°rio)
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

**Valida√ß√£o final:**
```sql
-- Rode no BigQuery para validar
SELECT COUNT(*) FROM `project.dataset.ga4_sessions`;
SELECT COUNT(*) FROM `project.dataset.google_ads_campaigns`;
-- Deve retornar > 0 rows
```

**Nomenclatura de vari√°veis:**
```python
# ‚úÖ CORRETO
DAYXX_GA4_PROPERTY_ID = "12345"
DAYXX_ADS_CUSTOMER_ID = "678-901-2345"

class dayXX_GA4Extractor:
    pass

def dayXX_process_ga4_sessions():
    pass

# ‚ùå ERRADO (causa conflitos)
GA4_PROPERTY_ID = "12345"
class GA4Extractor:
    pass
```

---

### **Day 1B (Samira - Instagram Creator)**

**Fonte de dados:**
- Instagram dados sint√©ticos (CSV gerado manualmente)

**Output obrigat√≥rio:**
- [ ] CSV sint√©tico: `instagram_posts.csv` com: `post_id, date, likes, comments, reach, post_type, caption_preview`
- [ ] Tabela `instagram_engagement` em BigQuery
- [ ] Engagement rate calculado: `(likes + comments) / reach`
- [ ] Schema documentado

**Quando parar:**
- ‚úÖ CSV sint√©tico com 30-50 posts (mix de imagens/reels/carousels)
- ‚úÖ Dados em BigQuery com engagement_rate calculado
- ‚úÖ README explica estrutura de dados e como adicionar mais posts
- ‚ùå N√ÉO fa√ßa: An√°lise de "qual post performa melhor", gr√°ficos, predictions

**Arquivos esperados:**
```
dayXX/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ instagram_synthetic.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îî‚îÄ‚îÄ instagram_engagement.csv
‚îú‚îÄ‚îÄ dayXX_DATA_generate_synthetic.py (cria o CSV)
‚îú‚îÄ‚îÄ dayXX_DATA_load_bigquery.py
‚îú‚îÄ‚îÄ dayXX_CONFIG_settings.py
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

---

### **Day 1C (Maru - GDPR Lead Ingestion)**

**Fonte de dados:**
- Webhook local (Flask) + leads sint√©ticos (JSON payloads)

**Output obrigat√≥rio:**
- [ ] Webhook Flask rodando em `localhost:5000/leads`
- [ ] Tabela `gdpr_leads` em BigQuery: `lead_id, name, email, consent_timestamp, consent_purpose, ip_address, data_retention_date`
- [ ] Exemplo de payload JSON no README
- [ ] L√≥gica de retention: `data_retention_date = consent_timestamp + 30 days` se n√£o consentido

**Quando parar:**
- ‚úÖ Webhook recebe POST request com JSON
- ‚úÖ Valida campos obrigat√≥rios (consent_timestamp, purpose)
- ‚úÖ Salva no BigQuery com metadata GDPR
- ‚úÖ README tem exemplo curl para testar
- ‚ùå N√ÉO fa√ßa: UI do formul√°rio, valida√ß√£o complexa de email, automa√ß√£o de deletion

**Arquivos esperados:**
```
dayXX/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ sample_payloads/
‚îÇ       ‚îú‚îÄ‚îÄ lead_with_consent.json
‚îÇ       ‚îî‚îÄ‚îÄ lead_without_consent.json
‚îú‚îÄ‚îÄ dayXX_APP_webhook_server.py (Flask app)
‚îú‚îÄ‚îÄ dayXX_PIPELINE_gdpr_validator.py
‚îú‚îÄ‚îÄ dayXX_DATA_load_bigquery.py
‚îú‚îÄ‚îÄ dayXX_CONFIG_settings.py
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

**Exemplo payload (README):**
```json
{
  "name": "Jo√£o Silva",
  "email": "joao@example.com",
  "consent_given": true,
  "consent_purpose": "marketing_communications",
  "ip_address": "192.168.1.1",
  "timestamp": "2024-11-26T10:30:00Z"
}
```

**Teste:**
```bash
curl -X POST http://localhost:5000/leads \
  -H "Content-Type: application/json" \
  -d @data/sample_payloads/lead_with_consent.json
```

**Nomenclatura:**
```python
# ‚úÖ CORRETO
DAYXX_WEBHOOK_PORT = 5000
DAYXX_GDPR_RETENTION_DAYS = 30

class dayXX_GDPRValidator:
    pass

def dayXX_calculate_retention_date():
    pass
```

---

### **Day 1D (Pedro - Crypto Price Tracker)**

**Fonte de dados:**
- CoinGecko API (free, no auth required)

**Output obrigat√≥rio:**
- [ ] Dockerfile funcional
- [ ] `docker-compose.yml` para rodar tudo
- [ ] Tabela `cardano_prices` em BigQuery: `timestamp, price_usd, market_cap, volume_24h, price_change_24h`
- [ ] Container executa, extrai dados, salva em BigQuery, e para

**Quando parar:**
- ‚úÖ `docker-compose up` funciona sem erros
- ‚úÖ Dados de Cardano (ADA) aparecem no BigQuery
- ‚úÖ README explica: build, run, environment variables
- ‚ùå N√ÉO fa√ßa: CI/CD complexo, m√∫ltiplas cryptos, an√°lise de tend√™ncias, scheduling

**Arquivos esperados:**
```
dayXX/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îî‚îÄ‚îÄ cardano_prices.csv (local backup)
‚îú‚îÄ‚îÄ dayXX_DATA_extract_coingecko.py
‚îú‚îÄ‚îÄ dayXX_DATA_load_bigquery.py
‚îú‚îÄ‚îÄ dayXX_CONFIG_settings.py
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

**Dockerfile (estrutura esperada):**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY dayXX_requirements.txt .
RUN pip install --no-cache-dir -r dayXX_requirements.txt
COPY dayXX_*.py ./
CMD ["python", "dayXX_DATA_extract_coingecko.py"]
```

**Teste:**
```bash
docker-compose up --build
# Deve ver logs: "Extracting Cardano prices... Done. Loaded to BigQuery."
```

**Nomenclatura:**
```python
# ‚úÖ CORRETO
DAYXX_COINGECKO_API_URL = "https://api.coingecko.com/api/v3"
DAYXX_CRYPTO_SYMBOL = "cardano"

class dayXX_CryptoExtractor:
    pass

def dayXX_fetch_cardano_price():
    pass
```

---

### **Day 1E (Paula - Museu Ipiranga Cultural Data)**

**Fonte de dados:**
- Tainacan REST API (Museu Paulista): `https://acervoonline.mp.usp.br/wp-json/tainacan/v2/`
- Spotify for Podcasters dados sint√©ticos (no real credentials)

**Output obrigat√≥rio:**
- [ ] Tabela `museum_artifacts` em BigQuery: `item_id, title, category, period, author, image_url, description`
- [ ] Tabela `podcast_episodes` (sint√©tico): `episode_id, title, publish_date, downloads, retention_rate, theme_category`
- [ ] Join logic preparado (theme_category comum entre tabelas)
- [ ] README explica estrutura de dados culturais

**Quando parar:**
- ‚úÖ 100-200 itens do acervo Ipiranga em BigQuery (via Tainacan API)
- ‚úÖ 10-15 epis√≥dios sint√©ticos do podcast
- ‚úÖ Categorias mapeadas (ex: "colonial_artifacts", "photography", "paintings")
- ‚úÖ README tem link para acervo online
- ‚ùå N√ÉO fa√ßa: An√°lise de correla√ß√£o tema-engagement, gr√°ficos, RAG sobre descri√ß√µes

**Arquivos esperados:**
```
dayXX/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tainacan_items.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ podcast_episodes_synthetic.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ museum_artifacts.csv
‚îÇ       ‚îî‚îÄ‚îÄ podcast_episodes.csv
‚îú‚îÄ‚îÄ dayXX_DATA_extract_tainacan.py
‚îú‚îÄ‚îÄ dayXX_DATA_generate_podcast.py (synthetic)
‚îú‚îÄ‚îÄ dayXX_DATA_load_bigquery.py
‚îú‚îÄ‚îÄ dayXX_CONFIG_settings.py
‚îú‚îÄ‚îÄ .env.example
‚îî‚îÄ‚îÄ README.md
```

**Tainacan API exemplo:**
```python
import requests

# ‚úÖ CORRETO - com nomenclatura day-scoped
DAYXX_TAINACAN_API_URL = "https://acervoonline.mp.usp.br/wp-json/tainacan/v2/"

def dayXX_fetch_museum_items():
    response = requests.get(
        f"{DAYXX_TAINACAN_API_URL}/items",
        params={"perpage": 100}
    )
    return response.json()
```

---

## üîÑ REGRA DE PIVOT (1 hora)

**Para Day 1A especificamente:**

Se ap√≥s **1 hora** voc√™ ainda n√£o conseguiu:
- Configurar GA4 Demo Account OU
- Extrair dados reais da API

‚û°Ô∏è **PIVOT IMEDIATO para dados sint√©ticos:**

1. Crie `data/raw/ga4_synthetic.csv`:
```csv
date,sessions,conversions,bounce_rate,source
2024-11-01,1250,45,0.42,google
2024-11-02,1180,38,0.45,facebook
2024-11-03,1320,52,0.39,direct
...
```

2. Crie `data/raw/ads_synthetic.csv`:
```csv
date,campaign_name,spend,clicks,impressions,conversions
2024-11-01,Brand Campaign,450.00,320,12500,18
2024-11-02,Product Launch,680.00,510,18200,25
...
```

3. Ajuste `dayXX_DATA_extract.py` para ler CSVs locais
4. Continue normalmente (load ‚Üí BigQuery ‚Üí README)

**N√ÉO gaste mais de 1h tentando fazer APIs reais funcionarem. O objetivo √© entregar, n√£o √© perfeccionismo.**

---

## üìä VALIDA√á√ÉO FINAL (Todos os Projetos)

Antes de dar push:
```bash
# 1. Teste em ambiente limpo
cd /tmp
git clone seu-repo
cd advent-automation-2025/dayXX

# 2. Configure .env (adicione vari√°veis do projeto ao root config/.env)
# Certifique-se de seguir conven√ß√£o: KEY_OPENAI_DAYXX, DAYXX_SPECIFIC_VAR

# 3. Execute
python dayXX_DATA_extract.py
python dayXX_DATA_load.py

# 4. Valide
# - Dados apareceram em BigQuery? ‚úÖ
# - Logs fazem sentido? ‚úÖ
# - Erros s√£o informativos? ‚úÖ
# - Nomenclatura est√° correta (dayXX_ prefix)? ‚úÖ
```

Se TUDO acima funciona ‚Üí ‚úÖ **Projeto completo**

---

## üîß CHECKLIST DE INTEGRA√á√ÉO COM ESTRUTURA EXISTENTE

Antes de finalizar qualquer projeto de Ingestion, verifique:

- [ ] Vari√°veis de ambiente adicionadas a `config/.env` seguindo conven√ß√£o existente
- [ ] Depend√™ncias espec√≠ficas documentadas em `dayXX_requirements.txt` (se necess√°rio)
- [ ] Depend√™ncias comuns adicionadas ao root `requirements.txt` (apenas se globalmente relevantes)
- [ ] TODOS os arquivos t√™m prefixo `dayXX_`
- [ ] TODAS as vari√°veis/classes/fun√ß√µes seguem nomenclatura isolada
- [ ] README explica claramente como configurar vari√°veis de ambiente
- [ ] Projeto funciona de forma INDEPENDENTE (n√£o depende de outros days)

---

## üí° LEMBRETE FINAL

**Voc√™ est√° construindo um PORTFOLIO, n√£o um produto de produ√ß√£o.**

O objetivo √© demonstrar:
- ‚úÖ Voc√™ sabe usar APIs
- ‚úÖ Voc√™ sabe estruturar dados
- ‚úÖ Voc√™ sabe documentar
- ‚úÖ Voc√™ consegue entregar em 3h
- ‚úÖ Voc√™ sabe trabalhar com isolamento de c√≥digo

**N√ÉO √© demonstrar:**
- ‚ùå Sistema perfeito e bug-free
- ‚ùå Performance otimizada
- ‚ùå Todos os edge cases tratados

**Foco: Funcional > Perfeito. Documentado > Complexo. Entregue > Ideal. Isolado > Compartilhado.**
